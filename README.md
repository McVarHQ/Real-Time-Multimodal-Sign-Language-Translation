# Real-Time Multimodal Sign Language Detection

---

## ğŸ” Overview

**Real-Time Multimodal Sign Language Detection** is a proof-of-concept prototype for **American Sign Language (ASL) translation**.  
The system leverages **ESP32-based wireless communication** with a **Raspberry Pi**, which runs a lightweight deep learning model to detect and translate ASL gestures in real-time.  

This project anticipates integration into wearable devices (e.g., **smartwatches, AR glasses**) for fluid and non-intrusive sign language translation.  
The key challenge addressed was designing and optimizing a model that is both **compact** and **accurate**, ensuring deployment feasibility on **resource-constrained SoCs**.

---

## ğŸ“œ Abstract

Sign language serves as a vital communication medium for the Deaf and Hard of Hearing (DHH) community. However, real-time and mobile sign-to-speech translation remains an open challenge.  

This project demonstrates a multimodal framework:  
- **Data Acquisition**: Leveraged WLASL dataset for word-level ASL recognition.  
- **Feature Extraction**: Used **pose landmarks** and skeletal keypoints from video sequences.  
- **Model Development**: Built and evaluated models combining temporal and spatial features, optimized for embedded deployment.  
- **Deployment**: The trained model was integrated on a Raspberry Pi prototype communicating with ESP32, demonstrating end-to-end wireless translation.  

The system proves the viability of **edge-deployable ASL recognition** and sets the groundwork for wearable sign language interpreters.

---

## âš™ï¸ Core Components

### 1. Dataset: WLASL
- Utilized **Word-Level American Sign Language (WLASL)** dataset.  
- Processed with subsets of **100, 300, 1000, and 2000 classes** (`nslt_*.json`).  
- Preprocessing steps included landmark extraction, missing data checks, and label formatting.  

### 2. Landmark Extraction
- Extracted **pose and hand landmarks** from ASL videos.  
- Generated intermediate representations for model training.  
- Landmarks stored in `.npz` and text formats (`labels.npz`, `final_labels.txt`).  

### 3. Model Development
- Built multimodal translation pipelines in:  
  - `mutemotion-wlasl-translation-model.ipynb`  
  - `Signer2.ipynb`  
- Employed **temporal sequence learning** for motion dynamics.  
- Optimized models for **low-memory footprint**, targeting ESP32-class hardware.  

### 4. Deployment & Communication
- **ESP32 module** captured data wirelessly.  
- **Raspberry Pi** processed inference using the trained model.  
- Designed for future portability to wearable form factors.  

---

## ğŸ¥ Demonstration

Watch the prototype in action:  
[![21BAI1499 | Real Time Multimodal Sign Language Detection Demo](https://img.youtube.com/vi/StTqXEQ2l-Y/0.jpg)](https://www.youtube.com/watch?v=q9amnUtUOjI "21BAI1499 | Real Time Multimodal Sign Language Detection Demo")


---

## ğŸ“‚ Repository Structure

```
SIGNER_COLAB/
â”‚
â”œâ”€â”€ ğŸ“ wlasl-complete/
â”‚   â”œâ”€â”€ ğŸ find_missing.py
â”‚   â”œâ”€â”€ ğŸ“„ missing.txt
â”‚   â”œâ”€â”€ ğŸ“‹ nslt_100.json
â”‚   â”œâ”€â”€ ğŸ“‹ nslt_300.json
â”‚   â”œâ”€â”€ ğŸ“‹ nslt_1000.json
â”‚   â”œâ”€â”€ ğŸ“‹ nslt_2000.json
â”‚   â””â”€â”€ ğŸ“„ wlasl_class_list.txt
â”‚
â”œâ”€â”€ ğŸ“ working/
â”‚   â”œâ”€â”€ ğŸ“ landmarks/
â”‚   â”œâ”€â”€ ğŸ“„ final_labels.txt
â”‚   â”œâ”€â”€ ğŸ“„ labels_2
â”‚   â”œâ”€â”€ ğŸ“„ labels.npz
â”‚   â”œâ”€â”€ ğŸ¥ landmarks_test.mp4
â”‚   â”œâ”€â”€ ğŸ““ mutemotion-wasl-translation-model.ipynb
â”‚   â””â”€â”€ ğŸ““ Signer2.ipynb
```

### ğŸ“‹ Directory Overview

| Directory/File | Description |
|----------------|-------------|
| `wlasl-complete/` | WLASL (Word-Level American Sign Language) dataset files |
| `â”œâ”€â”€ find_missing.py` | Python script to identify missing data |
| `â”œâ”€â”€ missing.txt` | List of missing entries |
| `â”œâ”€â”€ nslt_*.json` | NSLT datasets with different vocabulary sizes (100, 300, 1000, 2000) |
| `â””â”€â”€ wlasl_class_list.txt` | Complete class list for WLASL dataset |
| `working/` | Active development and processing files |
| `â”œâ”€â”€ landmarks/` | Extracted landmark data directory |
| `â”œâ”€â”€ final_labels.txt` | Processed label files |
| `â”œâ”€â”€ labels_2` & `labels.npz` | Label data in different formats |
| `â”œâ”€â”€ landmarks_test.mp4` | Test video for landmark extraction |
| `â””â”€â”€ *.ipynb` | Jupyter notebooks for model development |

---

## âœ… Key Highlights
- **Multimodal approach** using landmarks + video dynamics.  
- **Lightweight architecture** designed for **resource-constrained devices**.  
- **Prototype deployment** on ESP32 + Raspberry Pi.  
- Anticipates **wearable integration** (watch, AR glasses) for fluid real-time translation.  

---
